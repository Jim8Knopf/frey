#!/usr/bin/env python3
"""
Ollama Client - LLM interaction for command understanding and conversation.
"""

import json
import logging
import httpx
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)


class OllamaClient:
    """Client for Ollama LLM API."""

    def __init__(self, host: str, port: int, model: str):
        """
        Initialize Ollama client.

        Args:
            host: Ollama service hostname
            port: Ollama service port
            model: Model name to use
        """
        self.base_url = f"http://{host}:{port}"
        self.model = model
        self.client = httpx.AsyncClient(timeout=60.0)

        # System prompt for the voice assistant
        self.system_prompt = """You are Frey, a helpful voice assistant running on a Raspberry Pi server.
You can control Docker services, check system status, and answer questions.

Your capabilities:
- Start/stop Docker containers (e.g., "start jellyfin", "stop all media services")
- Check service status (e.g., "is sonarr running?", "list running services")
- Get system information (e.g., "what's the CPU usage?", "how much memory is free?")
- Answer general questions

Always respond concisely since your responses will be spoken aloud.
Keep responses under 2-3 sentences when possible.
For commands, confirm what you're doing (e.g., "Starting Jellyfin now").
"""

        logger.info(f"Ollama client initialized with model: {model}")

    async def ensure_model(self):
        """Ensure the model is pulled and available."""
        try:
            # Check if model exists
            response = await self.client.post(
                f"{self.base_url}/api/tags",
                timeout=10.0
            )

            if response.status_code == 200:
                models = response.json().get('models', [])
                model_exists = any(m.get('name') == self.model for m in models)

                if not model_exists:
                    logger.info(f"Pulling model {self.model}...")
                    await self.pull_model()
            else:
                logger.warning(f"Could not check models: {response.status_code}")

        except Exception as e:
            logger.warning(f"Model check failed: {e}")

    async def pull_model(self):
        """Pull the configured model."""
        try:
            logger.info(f"Pulling model: {self.model}")

            async with self.client.stream(
                'POST',
                f"{self.base_url}/api/pull",
                json={'name': self.model},
                timeout=600.0  # 10 minutes for model pull
            ) as response:
                async for line in response.aiter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            if 'status' in data:
                                logger.info(f"  {data['status']}")
                        except json.JSONDecodeError:
                            pass

            logger.info(f"Model {self.model} pulled successfully")

        except Exception as e:
            logger.error(f"Failed to pull model: {e}")
            raise

    async def process_command(self, user_input: str, command_handler) -> str:
        """
        Process user command using LLM and execute if needed.

        Args:
            user_input: User's voice command text
            command_handler: SystemCommandHandler instance

        Returns:
            Response text to speak
        """
        try:
            # First, let LLM understand the intent
            intent_response = await self.chat(
                user_input,
                context="Analyze this command and determine if it requires system action. "
                       "Respond with JSON: {\"action\": \"docker_start|docker_stop|docker_status|system_info|conversation\", "
                       "\"target\": \"service_name or 'all'\"}, \"response\": \"what to say to user\"}"
            )

            logger.debug(f"LLM intent response: {intent_response}")

            # Try to parse as JSON for structured commands
            try:
                intent = json.loads(intent_response)
                action = intent.get('action', 'conversation')
                target = intent.get('target', '')
                llm_response = intent.get('response', intent_response)

                # Execute system commands if needed
                if action == 'docker_start' and target:
                    result = command_handler.start_service(target)
                    return f"{llm_response} {result}"

                elif action == 'docker_stop' and target:
                    result = command_handler.stop_service(target)
                    return f"{llm_response} {result}"

                elif action == 'docker_status':
                    result = command_handler.get_service_status(target if target else None)
                    return f"{llm_response} {result}"

                elif action == 'system_info':
                    result = command_handler.get_system_info()
                    return f"{llm_response} {result}"

                else:
                    return llm_response

            except json.JSONDecodeError:
                # If not JSON, just return the LLM response
                return intent_response

        except Exception as e:
            logger.error(f"Command processing failed: {e}", exc_info=True)
            return "I'm sorry, I encountered an error processing that request."

    async def chat(self, message: str, context: Optional[str] = None) -> str:
        """
        Send a chat message to Ollama.

        Args:
            message: User message
            context: Optional context/instructions

        Returns:
            LLM response
        """
        try:
            # Build messages
            messages = [
                {'role': 'system', 'content': context or self.system_prompt},
                {'role': 'user', 'content': message}
            ]

            # Call Ollama API
            response = await self.client.post(
                f"{self.base_url}/api/chat",
                json={
                    'model': self.model,
                    'messages': messages,
                    'stream': False,
                    'options': {
                        'temperature': 0.7,
                        'top_p': 0.9,
                    }
                }
            )

            if response.status_code == 200:
                data = response.json()
                return data.get('message', {}).get('content', '').strip()
            else:
                logger.error(f"Ollama API error: {response.status_code}")
                return "I'm having trouble thinking right now."

        except Exception as e:
            logger.error(f"Chat failed: {e}", exc_info=True)
            return "I'm sorry, I couldn't process that."

    async def close(self):
        """Close the HTTP client."""
        await self.client.aclose()
